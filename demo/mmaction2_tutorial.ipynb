{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/open-mmlab/mmaction2/blob/master/demo/mmaction2_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcjSRFELVbNk"
   },
   "source": [
    "# MMAction2 Tutorial\n",
    "\n",
    "Welcome to MMAction2! This is the official colab tutorial for using MMAction2. In this tutorial, you will learn\n",
    "- Perform inference with a MMAction2 recognizer.\n",
    "- Train a new recognizer with a new dataset.\n",
    "- Perform spatio-temporal detection.\n",
    "\n",
    "Let's start!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LqHGkGEVqpm"
   },
   "source": [
    "## Install MMAction2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bf8PpPXtVvmg",
    "outputId": "75519a17-cc0a-491f-98a1-f287b090cf82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Jun__8_16:49:14_PDT_2022\n",
      "Cuda compilation tools, release 11.7, V11.7.99\n",
      "Build cuda_11.7.r11.7/compiler.31442593_0\n",
      "gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
      "Copyright (C) 2019 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check nvcc version\n",
    "!nvcc -V\n",
    "# Check GCC version\n",
    "!gcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mmaction',\n",
       " '.circleci',\n",
       " 'demo',\n",
       " 'checkpoints',\n",
       " '.readthedocs.yml',\n",
       " 'setup.cfg',\n",
       " 'docs',\n",
       " 'model-index.yml',\n",
       " 'tmp',\n",
       " 'README.md',\n",
       " 'requirements.txt',\n",
       " 'resources',\n",
       " '.gitignore',\n",
       " 'CITATION.cff',\n",
       " 'kinetics400_tiny',\n",
       " 'mmaction2.egg-info',\n",
       " '.owners.yml',\n",
       " 'docs_zh_CN',\n",
       " 'Untitled.ipynb',\n",
       " '.pre-commit-config.yaml',\n",
       " 'MANIFEST.in',\n",
       " 'work_dirs',\n",
       " '.ipynb_checkpoints',\n",
       " 'LICENSE',\n",
       " 'tests',\n",
       " '.pylintrc',\n",
       " 'tutorial_exps',\n",
       " 'configs',\n",
       " 'README_en-EN.md',\n",
       " 'requirements',\n",
       " 'README_zh-CN.md',\n",
       " 'docker',\n",
       " 'kinetics400_tiny.zip',\n",
       " 'tools',\n",
       " 'setup.py']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../../mmaction2\")\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5PAJ4ArzV5Ry",
    "outputId": "992b30c2-8281-4198-97c8-df2a287b0ae8"
   },
   "outputs": [],
   "source": [
    "# # install dependencies: (use cu111 because colab has CUDA 11.1)\n",
    "# !pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "# # install mmcv-full thus we could use CUDA operators\n",
    "# !pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.9.0/index.html\n",
    "\n",
    "# # Install mmaction2\n",
    "# !rm -rf mmaction2\n",
    "# !git clone https://github.com/open-mmlab/mmaction2.git\n",
    "# %cd mmaction2\n",
    "\n",
    "# !pip install -e .\n",
    "\n",
    "# Install some optional requirements\n",
    "# !pip install -r requirements/optional.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "No_zZAFpWC-a",
    "outputId": "1f5dd76e-7749-4fc3-ee97-83c5e1700f29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0+cu117 True\n",
      "0.24.1\n",
      "11.7\n",
      "GCC 9.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pidl/.local/lib/python3.8/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Check Pytorch installation\n",
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "\n",
    "# Check MMAction2 installation\n",
    "import mmaction\n",
    "print(mmaction.__version__)\n",
    "\n",
    "# Check MMCV installation\n",
    "from mmcv.ops import get_compiling_cuda_version, get_compiler_version\n",
    "print(get_compiling_cuda_version())\n",
    "print(get_compiler_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXf7oV5DWdab"
   },
   "source": [
    "## Perform inference with a MMAction2 recognizer\n",
    "MMAction2 already provides high level APIs to do inference and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64CW6d_AaT-Q",
    "outputId": "d08bfb9b-ab1e-451b-d3b2-89023a59766b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘checkpoints’: File exists\n",
      "--2022-12-06 20:29:08--  https://download.openmmlab.com/mmaction/recognition/tsn/tsn_r50_1x1x3_100e_kinetics400_rgb/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n",
      "Resolving download.openmmlab.com (download.openmmlab.com)... 47.75.20.5\n",
      "Connecting to download.openmmlab.com (download.openmmlab.com)|47.75.20.5|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "\n",
      "    The file is already fully retrieved; nothing to do.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir checkpoints\n",
    "!wget -c https://download.openmmlab.com/mmaction/recognition/tsn/tsn_r50_1x1x3_100e_kinetics400_rgb/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth \\\n",
    "      -O checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HNZB7NoSabzj",
    "outputId": "b2f9bd71-1490-44d3-81c6-5037d804f0b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import inference_recognizer, init_recognizer\n",
    "\n",
    "# Choose to use a config and initialize the recognizer\n",
    "config = 'configs/recognition/tsn/tsn_r50_video_inference_1x1x3_100e_kinetics400_rgb.py'\n",
    "# Setup a checkpoint file to load\n",
    "checkpoint = 'checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth'\n",
    "# Initialize the recognizer\n",
    "model = init_recognizer(config, checkpoint, device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rEMsBnpHapAn"
   },
   "outputs": [],
   "source": [
    "# Use the recognizer to do inference\n",
    "video = 'demo/demo.mp4'\n",
    "label = 'tools/data/kinetics/label_map_k400.txt'\n",
    "results = inference_recognizer(model, video)\n",
    "\n",
    "labels = open(label).readlines()\n",
    "labels = [x.strip() for x in labels]\n",
    "results = [(labels[k[0]], k[1]) for k in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NIyJXqfWathq",
    "outputId": "ca24528b-f99d-414a-fa50-456f6068b463"
   },
   "outputs": [],
   "source": [
    "# Let's show the results\n",
    "for result in results:\n",
    "    print(f'{result[0]}: ', result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QuZG8kZ2fJ5d"
   },
   "source": [
    "## Train a recognizer on customized dataset\n",
    "\n",
    "To train a new recognizer, there are usually three things to do:\n",
    "1. Support a new dataset\n",
    "2. Modify the config\n",
    "3. Train a new recognizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neEFyxChfgiJ"
   },
   "source": [
    "### Support a new dataset\n",
    "\n",
    "In this tutorial, we gives an example to convert the data into the format of existing datasets. Other methods and more advanced usages can be found in the [doc](/docs/tutorials/new_dataset.md)\n",
    "\n",
    "Firstly, let's download a tiny dataset obtained from [Kinetics-400](https://deepmind.com/research/open-source/open-source-datasets/kinetics/). We select 30 videos with their labels as train dataset and 10 videos with their labels as test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gjsUj9JzgUlJ",
    "outputId": "61c4704d-db81-4ca5-ed16-e2454dbdfe8e"
   },
   "outputs": [],
   "source": [
    "# download, decompress the data\n",
    "!rm kinetics400_tiny.zip*\n",
    "!rm -rf kinetics400_tiny\n",
    "!wget https://download.openmmlab.com/mmaction/kinetics400_tiny.zip\n",
    "!unzip kinetics400_tiny.zip > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AbZ-o7V6hNw4",
    "outputId": "b091909c-def2-49b5-88c2-01b00802b162"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n",
      "\u001b[01;34mkinetics400_tiny\u001b[00m\n",
      "├── kinetics_tiny_train_video.txt\n",
      "├── kinetics_tiny_val_video.txt\n",
      "├── \u001b[01;34mtrain\u001b[00m\n",
      "│   ├── \u001b[01;35m27_CSXByd3s.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35m34XczvTaRiI.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mA-wiliK50Zw.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mD32_1gwq35E.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mD92m0HsHjcQ.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mDbX8mPslRXg.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mFMlSTTpN3VY.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mh10B9SVE-nk.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mh2YqqUhnR34.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35miRuyZSKhHRg.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mIyfILH9lBRo.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mkFC3KY2bOP8.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mLvcFDgCAXQs.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mO46YA8tI530.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35moMrZaozOvdQ.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35moXy-e_P_cAI.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mP5M-hAts7MQ.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mphDqGd0NKoo.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mPnOe3GZRVX8.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mR8HXQkdgKWA.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mRqnKtCEoEcA.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35msoEcZZsBmDs.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mTkkZPZHbAKA.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mT_TMNGzVrDk.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mWaS0qwP46Us.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mWh_YPQdH1Zg.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mWWP5HZJsg-o.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35mxGY2dP0YUjA.mp4\u001b[00m\n",
      "│   ├── \u001b[01;35myLC9CtWU5ws.mp4\u001b[00m\n",
      "│   └── \u001b[01;35mZQV4U2KQ370.mp4\u001b[00m\n",
      "└── \u001b[01;34mval\u001b[00m\n",
      "    ├── \u001b[01;35m0pVGiAU6XEA.mp4\u001b[00m\n",
      "    ├── \u001b[01;35mAQrbRSnRt8M.mp4\u001b[00m\n",
      "    ├── \u001b[01;35mb6Q_b7vgc7Q.mp4\u001b[00m\n",
      "    ├── \u001b[01;35mddvJ6-faICE.mp4\u001b[00m\n",
      "    ├── \u001b[01;35mIcLztCtvhb8.mp4\u001b[00m\n",
      "    ├── \u001b[01;35mik4BW3-SCts.mp4\u001b[00m\n",
      "    ├── \u001b[01;35mjqRrH30V0k4.mp4\u001b[00m\n",
      "    ├── \u001b[01;35mSU_x2LQqSLs.mp4\u001b[00m\n",
      "    ├── \u001b[01;35mu4Rm6srmIS8.mp4\u001b[00m\n",
      "    └── \u001b[01;35my5Iu7XkTqV0.mp4\u001b[00m\n",
      "\n",
      "2 directories, 42 files\n"
     ]
    }
   ],
   "source": [
    "# Check the directory structure of the tiny data\n",
    "# Install tree first\n",
    "!apt-get -q install tree\n",
    "!tree kinetics400_tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fTdi6dI0hY3g",
    "outputId": "ffda0997-8d77-431a-d66e-2f273e80c756"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D32_1gwq35E.mp4 0\r\n",
      "iRuyZSKhHRg.mp4 1\r\n",
      "oXy-e_P_cAI.mp4 0\r\n",
      "34XczvTaRiI.mp4 1\r\n",
      "h2YqqUhnR34.mp4 0\r\n",
      "O46YA8tI530.mp4 0\r\n",
      "kFC3KY2bOP8.mp4 1\r\n",
      "WWP5HZJsg-o.mp4 1\r\n",
      "phDqGd0NKoo.mp4 1\r\n",
      "yLC9CtWU5ws.mp4 0\r\n",
      "27_CSXByd3s.mp4 1\r\n",
      "IyfILH9lBRo.mp4 1\r\n",
      "T_TMNGzVrDk.mp4 1\r\n",
      "TkkZPZHbAKA.mp4 0\r\n",
      "PnOe3GZRVX8.mp4 1\r\n",
      "soEcZZsBmDs.mp4 1\r\n",
      "FMlSTTpN3VY.mp4 1\r\n",
      "WaS0qwP46Us.mp4 0\r\n",
      "A-wiliK50Zw.mp4 1\r\n",
      "oMrZaozOvdQ.mp4 1\r\n",
      "ZQV4U2KQ370.mp4 0\r\n",
      "DbX8mPslRXg.mp4 1\r\n",
      "h10B9SVE-nk.mp4 1\r\n",
      "P5M-hAts7MQ.mp4 0\r\n",
      "R8HXQkdgKWA.mp4 0\r\n",
      "D92m0HsHjcQ.mp4 0\r\n",
      "RqnKtCEoEcA.mp4 0\r\n",
      "LvcFDgCAXQs.mp4 0\r\n",
      "xGY2dP0YUjA.mp4 0\r\n",
      "Wh_YPQdH1Zg.mp4 0\r\n"
     ]
    }
   ],
   "source": [
    "# After downloading the data, we need to check the annotation format\n",
    "!cat kinetics400_tiny/kinetics_tiny_train_video.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bq0mxmEi29H"
   },
   "source": [
    "According to the format defined in [`VideoDataset`](./datasets/video_dataset.py), each line indicates a sample video with the filepath and label, which are split with a whitespace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ht_DGJA9jQar"
   },
   "source": [
    "### Modify the config\n",
    "\n",
    "In the next step, we need to modify the config for the training.\n",
    "To accelerate the process, we finetune a recognizer using a pre-trained recognizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "LjCcmCKOjktc"
   },
   "outputs": [],
   "source": [
    "from mmcv import Config\n",
    "cfg = Config.fromfile('configs/recognition/tsn/tsn_r50_video_1x1x8_100e_kinetics400_rgb.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tc8YhFFGjp3e"
   },
   "source": [
    "Given a config that trains a TSN model on kinetics400-full dataset, we need to modify some values to use it for training TSN on Kinetics400-tiny dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tlhu9byjjt-K",
    "outputId": "3b9a3c49-ace0-41d3-dd15-d6c8579755f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='Recognizer2D',\n",
      "    backbone=dict(\n",
      "        type='ResNet',\n",
      "        pretrained='torchvision://resnet50',\n",
      "        depth=50,\n",
      "        norm_eval=False),\n",
      "    cls_head=dict(\n",
      "        type='TSNHead',\n",
      "        num_classes=2,\n",
      "        in_channels=2048,\n",
      "        spatial_type='avg',\n",
      "        consensus=dict(type='AvgConsensus', dim=1),\n",
      "        dropout_ratio=0.4,\n",
      "        init_std=0.01),\n",
      "    train_cfg=None,\n",
      "    test_cfg=dict(average_clips=None))\n",
      "optimizer = dict(type='SGD', lr=7.8125e-05, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))\n",
      "lr_config = dict(policy='step', step=[40, 80])\n",
      "total_epochs = 10\n",
      "checkpoint_config = dict(interval=5)\n",
      "log_config = dict(interval=5, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = 'checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "opencv_num_threads = 0\n",
      "mp_start_method = 'fork'\n",
      "dataset_type = 'VideoDataset'\n",
      "data_root = 'kinetics400_tiny/train/'\n",
      "data_root_val = 'kinetics400_tiny/val/'\n",
      "ann_file_train = 'kinetics400_tiny/kinetics_tiny_train_video.txt'\n",
      "ann_file_val = 'kinetics400_tiny/kinetics_tiny_val_video.txt'\n",
      "ann_file_test = 'kinetics400_tiny/kinetics_tiny_val_video.txt'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_bgr=False)\n",
      "train_pipeline = [\n",
      "    dict(type='DecordInit'),\n",
      "    dict(type='SampleFrames', clip_len=1, frame_interval=1, num_clips=8),\n",
      "    dict(type='DecordDecode'),\n",
      "    dict(\n",
      "        type='MultiScaleCrop',\n",
      "        input_size=224,\n",
      "        scales=(1, 0.875, 0.75, 0.66),\n",
      "        random_crop=False,\n",
      "        max_wh_scale_gap=1),\n",
      "    dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "    dict(type='Flip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "]\n",
      "val_pipeline = [\n",
      "    dict(type='DecordInit'),\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=1,\n",
      "        frame_interval=1,\n",
      "        num_clips=8,\n",
      "        test_mode=True),\n",
      "    dict(type='DecordDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='CenterCrop', crop_size=224),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='DecordInit'),\n",
      "    dict(\n",
      "        type='SampleFrames',\n",
      "        clip_len=1,\n",
      "        frame_interval=1,\n",
      "        num_clips=25,\n",
      "        test_mode=True),\n",
      "    dict(type='DecordDecode'),\n",
      "    dict(type='Resize', scale=(-1, 256)),\n",
      "    dict(type='TenCrop', crop_size=224),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_bgr=False),\n",
      "    dict(type='FormatShape', input_format='NCHW'),\n",
      "    dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "    dict(type='ToTensor', keys=['imgs'])\n",
      "]\n",
      "data = dict(\n",
      "    videos_per_gpu=2,\n",
      "    workers_per_gpu=2,\n",
      "    test_dataloader=dict(videos_per_gpu=1),\n",
      "    train=dict(\n",
      "        type='VideoDataset',\n",
      "        ann_file='kinetics400_tiny/kinetics_tiny_train_video.txt',\n",
      "        data_prefix='kinetics400_tiny/train/',\n",
      "        pipeline=[\n",
      "            dict(type='DecordInit'),\n",
      "            dict(\n",
      "                type='SampleFrames', clip_len=1, frame_interval=1,\n",
      "                num_clips=8),\n",
      "            dict(type='DecordDecode'),\n",
      "            dict(\n",
      "                type='MultiScaleCrop',\n",
      "                input_size=224,\n",
      "                scales=(1, 0.875, 0.75, 0.66),\n",
      "                random_crop=False,\n",
      "                max_wh_scale_gap=1),\n",
      "            dict(type='Resize', scale=(224, 224), keep_ratio=False),\n",
      "            dict(type='Flip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs', 'label'])\n",
      "        ]),\n",
      "    val=dict(\n",
      "        type='VideoDataset',\n",
      "        ann_file='kinetics400_tiny/kinetics_tiny_val_video.txt',\n",
      "        data_prefix='kinetics400_tiny/val/',\n",
      "        pipeline=[\n",
      "            dict(type='DecordInit'),\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=1,\n",
      "                frame_interval=1,\n",
      "                num_clips=8,\n",
      "                test_mode=True),\n",
      "            dict(type='DecordDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='CenterCrop', crop_size=224),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ]),\n",
      "    test=dict(\n",
      "        type='VideoDataset',\n",
      "        ann_file='kinetics400_tiny/kinetics_tiny_val_video.txt',\n",
      "        data_prefix='kinetics400_tiny/val/',\n",
      "        pipeline=[\n",
      "            dict(type='DecordInit'),\n",
      "            dict(\n",
      "                type='SampleFrames',\n",
      "                clip_len=1,\n",
      "                frame_interval=1,\n",
      "                num_clips=25,\n",
      "                test_mode=True),\n",
      "            dict(type='DecordDecode'),\n",
      "            dict(type='Resize', scale=(-1, 256)),\n",
      "            dict(type='TenCrop', crop_size=224),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_bgr=False),\n",
      "            dict(type='FormatShape', input_format='NCHW'),\n",
      "            dict(type='Collect', keys=['imgs', 'label'], meta_keys=[]),\n",
      "            dict(type='ToTensor', keys=['imgs'])\n",
      "        ]))\n",
      "evaluation = dict(\n",
      "    interval=5,\n",
      "    metrics=['top_k_accuracy', 'mean_class_accuracy'],\n",
      "    save_best='auto')\n",
      "work_dir = 'tutorial_exps'\n",
      "omnisource = False\n",
      "seed = 0\n",
      "gpu_ids = range(0, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmcv.runner import set_random_seed\n",
    "\n",
    "# Modify dataset type and path\n",
    "cfg.dataset_type = 'VideoDataset'\n",
    "cfg.data_root = 'kinetics400_tiny/train/'\n",
    "cfg.data_root_val = 'kinetics400_tiny/val/'\n",
    "cfg.ann_file_train = 'kinetics400_tiny/kinetics_tiny_train_video.txt'\n",
    "cfg.ann_file_val = 'kinetics400_tiny/kinetics_tiny_val_video.txt'\n",
    "cfg.ann_file_test = 'kinetics400_tiny/kinetics_tiny_val_video.txt'\n",
    "\n",
    "cfg.data.test.type = 'VideoDataset'\n",
    "cfg.data.test.ann_file = 'kinetics400_tiny/kinetics_tiny_val_video.txt'\n",
    "cfg.data.test.data_prefix = 'kinetics400_tiny/val/'\n",
    "\n",
    "cfg.data.train.type = 'VideoDataset'\n",
    "cfg.data.train.ann_file = 'kinetics400_tiny/kinetics_tiny_train_video.txt'\n",
    "cfg.data.train.data_prefix = 'kinetics400_tiny/train/'\n",
    "\n",
    "cfg.data.val.type = 'VideoDataset'\n",
    "cfg.data.val.ann_file = 'kinetics400_tiny/kinetics_tiny_val_video.txt'\n",
    "cfg.data.val.data_prefix = 'kinetics400_tiny/val/'\n",
    "\n",
    "# The flag is used to determine whether it is omnisource training\n",
    "cfg.setdefault('omnisource', False)\n",
    "# Modify num classes of the model in cls_head\n",
    "cfg.model.cls_head.num_classes = 2\n",
    "# We can use the pre-trained TSN model\n",
    "cfg.load_from = 'checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = 'tutorial_exps'\n",
    "\n",
    "# The original learning rate (LR) is set for 8-GPU training.\n",
    "# We divide it by 8 since we only use one GPU.\n",
    "cfg.data.videos_per_gpu = cfg.data.videos_per_gpu // 16\n",
    "cfg.optimizer.lr = cfg.optimizer.lr / 8 / 16\n",
    "cfg.total_epochs = 10\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 5\n",
    "# We can set the log print interval to reduce the the times of printing log\n",
    "cfg.log_config.interval = 5\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 0\n",
    "set_random_seed(0, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "# Save the best\n",
    "cfg.evaluation.save_best='auto'\n",
    "\n",
    "\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "print(f'Config:\\n{cfg.pretty_text}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tES-qnZ3k38Z"
   },
   "source": [
    "### Train a new recognizer\n",
    "\n",
    "Finally, lets initialize the dataset and recognizer, then train a new recognizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dDBWkdDRk6oz",
    "outputId": "a85d80d7-b3c4-43f1-d49a-057e8036807f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-06 20:30:00,808 - mmaction - INFO - These parameters in pretrained checkpoint are not loaded: {'fc.weight', 'fc.bias'}\n",
      "2022-12-06 20:30:00,866 - mmaction - INFO - load checkpoint from local path: checkpoints/tsn_r50_1x1x3_100e_kinetics400_rgb_20200614-e508be42.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from torchvision path: torchvision://resnet50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-06 20:30:00,955 - mmaction - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for cls_head.fc_cls.weight: copying a param with shape torch.Size([400, 2048]) from checkpoint, the shape in current model is torch.Size([2, 2048]).\n",
      "size mismatch for cls_head.fc_cls.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([2]).\n",
      "2022-12-06 20:30:00,961 - mmaction - INFO - Start running, host: pidl@pidl, work_dir: /home/pidl/working/mmtools/CoCoVa/mmaction2/tutorial_exps\n",
      "2022-12-06 20:30:00,962 - mmaction - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(LOW         ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(LOW         ) IterTimerHook                      \n",
      "(LOW         ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(LOW         ) IterTimerHook                      \n",
      "(LOW         ) EvalHook                           \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(LOW         ) IterTimerHook                      \n",
      "(LOW         ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(LOW         ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_run:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2022-12-06 20:30:00,964 - mmaction - INFO - workflow: [('train', 1)], max: 10 epochs\n",
      "2022-12-06 20:30:00,965 - mmaction - INFO - Checkpoints will be saved to /home/pidl/working/mmtools/CoCoVa/mmaction2/tutorial_exps by HardDiskBackend.\n",
      "/home/pidl/.local/lib/python3.8/site-packages/mmcv/runner/hooks/evaluation.py:226: UserWarning: runner.meta is None. Creating an empty one.\n",
      "  warnings.warn('runner.meta is None. Creating an empty one.')\n",
      "2022-12-06 20:30:05,866 - mmaction - INFO - Epoch [1][5/15]\tlr: 7.813e-05, eta: 0:02:21, time: 0.979, data_time: 0.519, memory: 1654, top1_acc: 0.5000, top5_acc: 1.0000, loss_cls: 0.6811, loss: 0.6811, grad_norm: 12.7995\n",
      "2022-12-06 20:30:06,793 - mmaction - INFO - Epoch [1][10/15]\tlr: 7.813e-05, eta: 0:01:21, time: 0.185, data_time: 0.002, memory: 1654, top1_acc: 0.6000, top5_acc: 1.0000, loss_cls: 0.7090, loss: 0.7090, grad_norm: 14.1347\n",
      "2022-12-06 20:30:07,720 - mmaction - INFO - Epoch [1][15/15]\tlr: 7.813e-05, eta: 0:01:00, time: 0.185, data_time: 0.001, memory: 1654, top1_acc: 0.1000, top5_acc: 1.0000, loss_cls: 0.7833, loss: 0.7833, grad_norm: 13.1844\n",
      "2022-12-06 20:30:11,247 - mmaction - INFO - Epoch [2][5/15]\tlr: 7.813e-05, eta: 0:01:06, time: 0.695, data_time: 0.509, memory: 1654, top1_acc: 0.6000, top5_acc: 1.0000, loss_cls: 0.6344, loss: 0.6344, grad_norm: 10.2298\n",
      "2022-12-06 20:30:12,243 - mmaction - INFO - Epoch [2][10/15]\tlr: 7.813e-05, eta: 0:00:56, time: 0.199, data_time: 0.014, memory: 1654, top1_acc: 0.3000, top5_acc: 1.0000, loss_cls: 0.7548, loss: 0.7548, grad_norm: 14.7548\n",
      "2022-12-06 20:30:13,262 - mmaction - INFO - Epoch [2][15/15]\tlr: 7.813e-05, eta: 0:00:48, time: 0.204, data_time: 0.019, memory: 1654, top1_acc: 0.6000, top5_acc: 1.0000, loss_cls: 0.6531, loss: 0.6531, grad_norm: 11.4076\n",
      "2022-12-06 20:30:16,719 - mmaction - INFO - Epoch [3][5/15]\tlr: 7.813e-05, eta: 0:00:51, time: 0.679, data_time: 0.493, memory: 1654, top1_acc: 0.7000, top5_acc: 1.0000, loss_cls: 0.7460, loss: 0.7460, grad_norm: 13.3508\n",
      "2022-12-06 20:30:17,651 - mmaction - INFO - Epoch [3][10/15]\tlr: 7.813e-05, eta: 0:00:45, time: 0.186, data_time: 0.001, memory: 1654, top1_acc: 0.6000, top5_acc: 1.0000, loss_cls: 0.6894, loss: 0.6894, grad_norm: 11.3398\n",
      "2022-12-06 20:30:18,580 - mmaction - INFO - Epoch [3][15/15]\tlr: 7.813e-05, eta: 0:00:40, time: 0.186, data_time: 0.001, memory: 1654, top1_acc: 0.5000, top5_acc: 1.0000, loss_cls: 0.7108, loss: 0.7108, grad_norm: 11.4166\n",
      "2022-12-06 20:30:22,068 - mmaction - INFO - Epoch [4][5/15]\tlr: 7.813e-05, eta: 0:00:41, time: 0.682, data_time: 0.496, memory: 1654, top1_acc: 0.5000, top5_acc: 1.0000, loss_cls: 0.6853, loss: 0.6853, grad_norm: 12.8871\n",
      "2022-12-06 20:30:23,044 - mmaction - INFO - Epoch [4][10/15]\tlr: 7.813e-05, eta: 0:00:37, time: 0.195, data_time: 0.010, memory: 1654, top1_acc: 0.6000, top5_acc: 1.0000, loss_cls: 0.7092, loss: 0.7092, grad_norm: 11.3048\n",
      "2022-12-06 20:30:24,095 - mmaction - INFO - Epoch [4][15/15]\tlr: 7.813e-05, eta: 0:00:34, time: 0.210, data_time: 0.024, memory: 1654, top1_acc: 0.7000, top5_acc: 1.0000, loss_cls: 0.6692, loss: 0.6692, grad_norm: 12.5633\n",
      "2022-12-06 20:30:27,792 - mmaction - INFO - Epoch [5][5/15]\tlr: 7.813e-05, eta: 0:00:34, time: 0.726, data_time: 0.537, memory: 1654, top1_acc: 0.3000, top5_acc: 1.0000, loss_cls: 0.6977, loss: 0.6977, grad_norm: 11.1036\n",
      "2022-12-06 20:30:28,726 - mmaction - INFO - Epoch [5][10/15]\tlr: 7.813e-05, eta: 0:00:31, time: 0.187, data_time: 0.001, memory: 1654, top1_acc: 0.7000, top5_acc: 1.0000, loss_cls: 0.6282, loss: 0.6282, grad_norm: 10.2517\n",
      "2022-12-06 20:30:29,661 - mmaction - INFO - Epoch [5][15/15]\tlr: 7.813e-05, eta: 0:00:28, time: 0.187, data_time: 0.001, memory: 1654, top1_acc: 0.7000, top5_acc: 1.0000, loss_cls: 0.6609, loss: 0.6609, grad_norm: 11.0814\n",
      "2022-12-06 20:30:29,717 - mmaction - INFO - Saving checkpoint at 5 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 10/10, 6.2 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-06 20:30:31,882 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2022-12-06 20:30:31,884 - mmaction - INFO - \n",
      "top1_acc\t0.7000\n",
      "top5_acc\t1.0000\n",
      "2022-12-06 20:30:31,885 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2022-12-06 20:30:31,886 - mmaction - INFO - \n",
      "mean_acc\t0.7000\n",
      "2022-12-06 20:30:32,324 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_5.pth.\n",
      "2022-12-06 20:30:32,325 - mmaction - INFO - Best top1_acc is 0.7000 at 5 epoch.\n",
      "2022-12-06 20:30:32,326 - mmaction - INFO - Epoch(val) [5][5]\ttop1_acc: 0.7000, top5_acc: 1.0000, mean_class_accuracy: 0.7000\n",
      "2022-12-06 20:30:35,869 - mmaction - INFO - Epoch [6][5/15]\tlr: 7.813e-05, eta: 0:00:27, time: 0.707, data_time: 0.516, memory: 1654, top1_acc: 0.7000, top5_acc: 1.0000, loss_cls: 0.6413, loss: 0.6413, grad_norm: 12.5305\n",
      "2022-12-06 20:30:36,804 - mmaction - INFO - Epoch [6][10/15]\tlr: 7.813e-05, eta: 0:00:25, time: 0.187, data_time: 0.001, memory: 1654, top1_acc: 0.5000, top5_acc: 1.0000, loss_cls: 0.6945, loss: 0.6945, grad_norm: 11.3710\n",
      "2022-12-06 20:30:37,742 - mmaction - INFO - Epoch [6][15/15]\tlr: 7.813e-05, eta: 0:00:22, time: 0.187, data_time: 0.001, memory: 1654, top1_acc: 0.6000, top5_acc: 1.0000, loss_cls: 0.6450, loss: 0.6450, grad_norm: 10.7346\n",
      "2022-12-06 20:30:41,350 - mmaction - INFO - Epoch [7][5/15]\tlr: 7.813e-05, eta: 0:00:21, time: 0.708, data_time: 0.520, memory: 1654, top1_acc: 0.5000, top5_acc: 1.0000, loss_cls: 0.6379, loss: 0.6379, grad_norm: 10.7371\n",
      "2022-12-06 20:30:42,325 - mmaction - INFO - Epoch [7][10/15]\tlr: 7.813e-05, eta: 0:00:19, time: 0.195, data_time: 0.008, memory: 1654, top1_acc: 0.6000, top5_acc: 1.0000, loss_cls: 0.6858, loss: 0.6858, grad_norm: 13.2679\n",
      "2022-12-06 20:30:43,263 - mmaction - INFO - Epoch [7][15/15]\tlr: 7.813e-05, eta: 0:00:16, time: 0.188, data_time: 0.001, memory: 1654, top1_acc: 0.8000, top5_acc: 1.0000, loss_cls: 0.6632, loss: 0.6632, grad_norm: 10.9742\n",
      "2022-12-06 20:30:46,782 - mmaction - INFO - Epoch [8][5/15]\tlr: 7.813e-05, eta: 0:00:15, time: 0.690, data_time: 0.502, memory: 1654, top1_acc: 0.6000, top5_acc: 1.0000, loss_cls: 0.6295, loss: 0.6295, grad_norm: 11.3371\n",
      "2022-12-06 20:30:47,720 - mmaction - INFO - Epoch [8][10/15]\tlr: 7.813e-05, eta: 0:00:13, time: 0.188, data_time: 0.002, memory: 1654, top1_acc: 0.9000, top5_acc: 1.0000, loss_cls: 0.6112, loss: 0.6112, grad_norm: 10.6512\n",
      "2022-12-06 20:30:48,655 - mmaction - INFO - Epoch [8][15/15]\tlr: 7.813e-05, eta: 0:00:11, time: 0.187, data_time: 0.001, memory: 1654, top1_acc: 0.7000, top5_acc: 1.0000, loss_cls: 0.5998, loss: 0.5998, grad_norm: 9.4665\n",
      "2022-12-06 20:30:52,117 - mmaction - INFO - Epoch [9][5/15]\tlr: 7.813e-05, eta: 0:00:09, time: 0.679, data_time: 0.491, memory: 1654, top1_acc: 0.7000, top5_acc: 1.0000, loss_cls: 0.6198, loss: 0.6198, grad_norm: 10.1256\n",
      "2022-12-06 20:30:53,271 - mmaction - INFO - Epoch [9][10/15]\tlr: 7.813e-05, eta: 0:00:07, time: 0.231, data_time: 0.044, memory: 1654, top1_acc: 0.9000, top5_acc: 1.0000, loss_cls: 0.5605, loss: 0.5605, grad_norm: 8.9696\n",
      "2022-12-06 20:30:54,206 - mmaction - INFO - Epoch [9][15/15]\tlr: 7.813e-05, eta: 0:00:05, time: 0.187, data_time: 0.001, memory: 1654, top1_acc: 0.6000, top5_acc: 1.0000, loss_cls: 0.6036, loss: 0.6036, grad_norm: 10.4462\n",
      "2022-12-06 20:30:57,675 - mmaction - INFO - Epoch [10][5/15]\tlr: 7.813e-05, eta: 0:00:03, time: 0.682, data_time: 0.495, memory: 1654, top1_acc: 0.8000, top5_acc: 1.0000, loss_cls: 0.6408, loss: 0.6408, grad_norm: 12.3329\n",
      "2022-12-06 20:30:58,614 - mmaction - INFO - Epoch [10][10/15]\tlr: 7.813e-05, eta: 0:00:01, time: 0.188, data_time: 0.002, memory: 1654, top1_acc: 0.8000, top5_acc: 1.0000, loss_cls: 0.6086, loss: 0.6086, grad_norm: 10.4391\n",
      "2022-12-06 20:30:59,552 - mmaction - INFO - Epoch [10][15/15]\tlr: 7.813e-05, eta: 0:00:00, time: 0.188, data_time: 0.001, memory: 1654, top1_acc: 0.5000, top5_acc: 1.0000, loss_cls: 0.6768, loss: 0.6768, grad_norm: 11.3197\n",
      "2022-12-06 20:30:59,608 - mmaction - INFO - Saving checkpoint at 10 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 10/10, 6.4 task/s, elapsed: 2s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-06 20:31:01,658 - mmaction - INFO - Evaluating top_k_accuracy ...\n",
      "2022-12-06 20:31:01,660 - mmaction - INFO - \n",
      "top1_acc\t0.8000\n",
      "top5_acc\t1.0000\n",
      "2022-12-06 20:31:01,661 - mmaction - INFO - Evaluating mean_class_accuracy ...\n",
      "2022-12-06 20:31:01,662 - mmaction - INFO - \n",
      "mean_acc\t0.8000\n",
      "2022-12-06 20:31:01,701 - mmaction - INFO - The previous best checkpoint /home/pidl/working/mmtools/CoCoVa/mmaction2/tutorial_exps/best_top1_acc_epoch_5.pth was removed\n",
      "2022-12-06 20:31:02,117 - mmaction - INFO - Now best checkpoint is saved as best_top1_acc_epoch_10.pth.\n",
      "2022-12-06 20:31:02,119 - mmaction - INFO - Best top1_acc is 0.8000 at 10 epoch.\n",
      "2022-12-06 20:31:02,120 - mmaction - INFO - Epoch(val) [10][5]\ttop1_acc: 0.8000, top5_acc: 1.0000, mean_class_accuracy: 0.8000\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "from mmaction.datasets import build_dataset\n",
    "from mmaction.models import build_model\n",
    "from mmaction.apis import train_model\n",
    "\n",
    "import mmcv\n",
    "\n",
    "# Build the dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# Build the recognizer\n",
    "model = build_model(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "\n",
    "# Create work_dir\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_model(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdSd7oTLlxIf"
   },
   "source": [
    "### Understand the log\n",
    "From the log, we can have a basic understanding the training process and know how well the recognizer is trained.\n",
    "\n",
    "Firstly, the ResNet-50 backbone pre-trained on ImageNet is loaded, this is a common practice since training from scratch is more cost. The log shows that all the weights of the ResNet-50 backbone are loaded except the `fc.bias` and `fc.weight`.\n",
    "\n",
    "Second, since the dataset we are using is small, we loaded a TSN model and finetune it for action recognition.\n",
    "The original TSN is trained on original Kinetics-400 dataset which contains 400 classes but Kinetics-400 Tiny dataset only have 2 classes. Therefore, the last FC layer of the pre-trained TSN for classification has different weight shape and is not used.\n",
    "\n",
    "Third, after training, the recognizer is evaluated by the default evaluation. The results show that the recognizer achieves 100% top1 accuracy and 100% top5 accuracy on the val dataset,\n",
    " \n",
    "Not bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryVoSfZVmogw"
   },
   "source": [
    "## Test the trained recognizer\n",
    "\n",
    "After finetuning the recognizer, let's check the prediction results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyY3hCMwyTct",
    "outputId": "ea54ff0a-4299-4e93-c1ca-4fe597e7516b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 10/10, 1.1 task/s, elapsed: 9s, ETA:     0s\n",
      "Evaluating top_k_accuracy ...\n",
      "\n",
      "top1_acc\t0.8000\n",
      "top5_acc\t1.0000\n",
      "\n",
      "Evaluating mean_class_accuracy ...\n",
      "\n",
      "mean_acc\t0.8000\n",
      "top1_acc: 0.8000\n",
      "top5_acc: 1.0000\n",
      "mean_class_accuracy: 0.8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pidl/working/mmtools/CoCoVa/mmaction2/mmaction/datasets/base.py:166: UserWarning: Option arguments for metrics has been changed to `metric_options`, See 'https://github.com/open-mmlab/mmaction2/pull/286' for more details\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import single_gpu_test\n",
    "from mmaction.datasets import build_dataloader\n",
    "from mmcv.parallel import MMDataParallel\n",
    "\n",
    "# Build a test dataloader\n",
    "dataset = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "data_loader = build_dataloader(\n",
    "        dataset,\n",
    "        videos_per_gpu=1,\n",
    "        workers_per_gpu=cfg.data.workers_per_gpu,\n",
    "        dist=False,\n",
    "        shuffle=False)\n",
    "model = MMDataParallel(model, device_ids=[0])\n",
    "outputs = single_gpu_test(model, data_loader)\n",
    "\n",
    "eval_config = cfg.evaluation\n",
    "eval_config.pop('interval')\n",
    "eval_res = dataset.evaluate(outputs, **eval_config)\n",
    "for name, val in eval_res.items():\n",
    "    print(f'{name}: {val:.04f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZ4t44nWmZDM"
   },
   "source": [
    "## Perform Spatio-Temporal Detection\n",
    "Here we first install MMDetection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w1p0_g76nHOQ",
    "outputId": "b30a6be3-c457-452e-c789-7083117c5011"
   },
   "outputs": [],
   "source": [
    "# Git clone mmdetection repo\n",
    "%cd ..\n",
    "!git clone https://github.com/open-mmlab/mmdetection.git\n",
    "%cd mmdetection\n",
    "\n",
    "# install mmdet\n",
    "!pip install -e .\n",
    "%cd ../mmaction2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlOQsH8OnVKn"
   },
   "source": [
    "Download a video to `demo` directory in MMAction2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QaW3jg5Enish",
    "outputId": "c70cde3a-b337-41d0-cb08-82dfc746d9ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-12-06 20:33:42--  https://download.openmmlab.com/mmaction/dataset/sample/1j20qq1JyX4.mp4\n",
      "Resolving download.openmmlab.com (download.openmmlab.com)... 47.75.20.5\n",
      "Connecting to download.openmmlab.com (download.openmmlab.com)|47.75.20.5|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4864186 (4.6M) [video/mp4]\n",
      "Saving to: ‘demo/1j20qq1JyX4.mp4’\n",
      "\n",
      "demo/1j20qq1JyX4.mp 100%[===================>]   4.64M  1.64MB/s    in 2.8s    \n",
      "\n",
      "2022-12-06 20:33:47 (1.64 MB/s) - ‘demo/1j20qq1JyX4.mp4’ saved [4864186/4864186]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://download.openmmlab.com/mmaction/dataset/sample/1j20qq1JyX4.mp4 -O demo/1j20qq1JyX4.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYGxdu8Vnoah"
   },
   "source": [
    "Run spatio-temporal demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LPLiaHaYnrb7",
    "outputId": "8a8f8a16-ad7b-4559-c19c-c8264533bff3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pidl/anaconda3/envs/mmtool/lib/python3.9/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n",
      "load checkpoint from local path: checkpoints/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth\n",
      "Performing Human Detection for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 217/217, 7.4 task/s, elapsed: 29s, ETA:     0sload checkpoint from local path: checkpoints/slowonly_omnisource_pretrained_r101_8x8x1_20e_ava_rgb_20201217-16378594.pth\n",
      "Performing SpatioTemporal Action Detection for each clip\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>       ] 167/217, 7.2 task/s, elapsed: 23s, ETA:     7sPerforming visualization\n",
      "Moviepy - Building video demo/stdet_demo.mp4.\n",
      "Moviepy - Writing video demo/stdet_demo.mp4\n",
      "\n",
      "Moviepy - Done !                                                                \n",
      "Moviepy - video ready demo/stdet_demo.mp4\n"
     ]
    }
   ],
   "source": [
    "!python demo/demo_spatiotemporal_det.py --video demo/1j20qq1JyX4.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "id": "-0atQCzBo9-C",
    "outputId": "b6bb3a67-669c-45d0-cdf4-25b6210362d0"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'demo/stdet_demo.mp4'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTML\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbase64\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m b64encode\n\u001b[0;32m----> 4\u001b[0m mp4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdemo/stdet_demo.mp4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m      5\u001b[0m data_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata:video/mp4;base64,\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m b64encode(mp4)\u001b[38;5;241m.\u001b[39mdecode()\n\u001b[1;32m      6\u001b[0m HTML(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m<video width=400 controls>\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m      <source src=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m type=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo/mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124m</video>\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m \u001b[38;5;241m%\u001b[39m data_url)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'demo/stdet_demo.mp4'"
     ]
    }
   ],
   "source": [
    "# Check the video\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "mp4 = open('demo/stdet_demo.mp4','rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "HTML(\"\"\"\n",
    "<video width=400 controls>\n",
    "      <source src=\"%s\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\" % data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "MMAction2 Tutorial.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
